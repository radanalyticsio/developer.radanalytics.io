= Ceph Source Example
:page-link: ceph-source-example
:page-weight: 100
:page-labels: [Python, Ceph, S3, Jupyter]
:page-layout: application
:page-menu_template: menu_tutorial_application.html
:page-description: This is an example of how to connect your application to data in Ceph using S3 API.
:page-project_links: ["https://github.com/rimolive/ceph-spark-integration"]

[[introduction]]
== Introduction

Processing data stored in an external object store is a practical and
popular way for an intelligent application to operate.

This is an example of the key pieces needed to connect your
application to data in Ceph using S3 API. It is presented as steps in a Jupyter
notebook. To run this example, you need a Ceph environment running in your network
and a running Spark cluster in your OpenShift project.
Optionally, you can run in your own computer using `vagrant` and `ansible`
(check the <<prerequisites>> section if you need to deploy a local instance of Ceph).

https://github.com/radanalyticsio/radanalyticsio.github.io/blob/master/assets/ceph-source-example/ceph-example.ipynb[Ceph Source Example]

https://github.com/ceph/ceph-ansible[Ceph Ansible playbooks]


[[prerequisites]]
== Prerequisites

In case you need to configure your own Ceph environment, follow the
steps below:

1. Clone https://github.com/ceph/ceph-ansible[ceph-ansible] GitHub repo.
2. Rename `vagrant_variables.yml.sample` to `vagrant_variables.yml`
   and edit the file by changing to the following snippet:

   ...
   # DEFINE THE NUMBER OF VMS TO RUN
   mon_vms: 1
   osd_vms: 3
   mds_vms: 0
   rgw_vms: 1
   ...

3. Rename also `site.yml.sample` to `site.yml`.
4. Run `vagrant up` inside the repo.
5. Follow the link:/get-started[getting started] instructions to create an OpenShift project with the Oshinko web interface running.

[[architecture]]
== Architecture

No architecture, this is a connectivity example.

[[installation]]
== Installation

1. From the OpenShift developer console, visit the Oshinko web interface. Use the 
   interface to create a new cluster, and take note of what youâ€™ve called this cluster.

2. Start a Jupyter notebook with the commands:

   oc new-app rimolive/notebook
   oc expose svc/notebook

3. Find the connection url using the following command:

   oc logs dc/pyspark-hdfs-notebook | grep localhost | sed "s/localhost:8888/$(oc get routes/pyspark-hdfs-notebook --template='{{.spec.host}}')/"

4. Download link:/assets/ceph-source-example/ceph-example.ipynb[the notebook], upload into jupyter and follow the notebook instructions. Make sure in the line where the notebook connects to the spark cluster you changed with the cluster name you used in step 1 and in the line where you set up the Ceph connection you change with the IP where you Ceph instance is running.

[[usage]]
== Usage

No specific usage.

[[expansion]]
== Expansion

No specific expansion.

[[videos]]
== Videos

No video, follow the notebook steps.
