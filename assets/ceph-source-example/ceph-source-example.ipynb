{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading input from Ceph Object Store with Apache Spark on OpenShift\n",
    "\n",
    "In this demonstration we will load textual data from [Ceph](http://ceph.com/) using [S3 API](http://docs.ceph.com/docs/master/radosgw/s3/). There are two key pieces of information to get from this demonstration,\n",
    "\n",
    "0. First, loading of the S3 client libraries (hadoop-aws)\n",
    "1. Second, configuring the client with Ceph/S3 credentials.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important - load the S3 client libraries\n",
    "\n",
    "This uses some Jupyter line magic to put **--packages** on the pyspark command line for the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%set_env PYSPARK_SUBMIT_ARGS=--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Notebook parameters\n",
    "\n",
    "The following parameters must be filled according to your environment. If you have a Ceph environment deployed using **Ceph Nano**, you can get the access keys and the ceph IP with the `cn cluster status <cluster-name>` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "access_key = '' # Add your S3 Access Key here\n",
    "secret_key = '' # Add your S3 Secret Key here\n",
    "bucket_name = \"ceph-source\"\n",
    "\n",
    "ceph_host = '' # Add your rgw0 Ceph host here\n",
    "ceph_port = 8000\n",
    "\n",
    "spark_cluster = '' # add your Spark Cluster name here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup a bucket\n",
    "\n",
    "Before create a job in our Spark cluster, let's create a bucket using S3 API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto\n",
    "import boto.s3.connection\n",
    "\n",
    "conn = boto.connect_s3(\n",
    "        aws_access_key_id = access_key,\n",
    "        aws_secret_access_key = secret_key,\n",
    "        host = ceph_host,\n",
    "        port = ceph_port,\n",
    "        is_secure=False,\n",
    "        calling_format = boto.s3.connection.OrdinaryCallingFormat()\n",
    "        )\n",
    "\n",
    "bucket = conn.create_bucket(\"my-bucket\")\n",
    "\n",
    "print \"Bucket {} created!\".format(\"my-bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put a file in the Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto.s3.key import Key\n",
    "\n",
    "object_key = \"spark-test\"\n",
    "object_value = \"/opt/spark/README.md\"\n",
    "\n",
    "bucket = conn.get_bucket(\"my-bucket\")\n",
    "\n",
    "k = Key(bucket)\n",
    "k.key = object_key\n",
    "k.set_contents_from_filename(object_value)\n",
    "\n",
    "print \"Object {} added in bucket {}!\".format(object_key, \"my-bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List contents in the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in bucket.list():\n",
    "    print \"{name}\\t{size}\\t{modified}\".format(\n",
    "        name = key.name,\n",
    "        size = key.size,\n",
    "        modified = key.last_modified,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup your SparkSession as you normally would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "conf=pyspark.SparkConf().setMaster('spark://{}:7077'.format(spark_cluster)) \\\n",
    "     .set('spark.driver.host', 'base-notebook') \\\n",
    "     .set('spark.driver.port', 42000) \\\n",
    "     .set('spark.driver.bindAddress', '0.0.0.0') \\\n",
    "     .set('spark.driver.blockManager.port', 42100)\n",
    "spark=pyspark.SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important - configure the S3 client with your credentials\n",
    "Don't store your credentials in code, use [Secrets](https://kubernetes.io/docs/user-guide/secrets/). Do use [AWS IAM](https://aws.amazon.com/iam/) and credentials with only the capabilities needed for your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf=spark._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.access.key\", access_key)\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", secret_key)\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", \"{}:8080\".format(ceph_host))\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple test to see what workers are available in your cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "spark._jsc.sc().getExecutorMemoryStatus().size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read a simple text file from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df0 = spark.textFile(\"s3a://ceph-source/ceph-source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.take(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "df0.flatMap(lambda x: list(x[0])).map(lambda x: (x, 1)).reduceByKey(add).sortBy(lambda x: x[1], ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = df0.flatMap(lambda line: line[0].split(\" \")) \\\n",
    "             .map(lambda word: (word, 1)) \\\n",
    "             .reduceByKey(add) \\\n",
    "             .take(10)\n",
    "counts"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
