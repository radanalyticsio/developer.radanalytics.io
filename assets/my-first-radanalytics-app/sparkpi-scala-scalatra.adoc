= SparkPi using Scala and Scalatra
:page-layout: markdown
:page-menu_template: menu_tutorial_application.html
:page-menu_backurl: /my-first-radanalytics-app.html
:page-menu_backtext: Back to My First RADanalytics Application

== Building a Spark PI Microservice with Scalatra

These instructions will help you to create a SparkPi microservice using https://www.scala-lang.org[Scala] and the http://scalatra.org[Scalatra] framework. In addition, this project will also demonstrate how to unit test your Spark jobs using the https://github.com/holdenk/spark-testing-base/wiki[Spark Testing Base] framework.

You should already have the necessary prerequisites installed and configured, but if not please review the link:/my-first-radanalytics-app.html[instructions]. In addition, this tutorial will assume you have used the Oshinko Web UI to create a Spark cluster named **spark**. This will be used as the **OSHINKO_CLUSTER_NAME** parameter.

== Create the Build

This project is built using https://www.scala-sbt.org/[SBT]. First, create the following file/directory structure

....
tutorial-sparkpi-scala-scalatra (root)
    \_ build.sbt
    \_ project
        \_ Dependencies.scala
        \_ build.properties
        \_ plugins.sbt
....

The primary build definition is contained in `build.sbt` (analogous to pom.xml for Maven or build.gradle for Gradle), however, there are also configurations that are applied project wide from the **project** directory. For an application as small and as simple as this, these configurations can be superfluous (ie: they could reside directly in the build.sbt file). However, these configurations become much more important in complex multi-module builds. Therefore, as a matter of best practice, this tutorial will be set up that way.

The file `build.properties` should be autogenerated for you if you are using an IDE like Eclipse or IntelliJ IDEA. However, in this case it will be created by hand. For now it simply contains the SBT version that the project is built with.

```scala
sbt.version = 1.0.2
```

The `plugins.sbt` file enables plugins globally for the project by adding extra settings and tasks. For this project, there are a few plugins required to enable the **Scalatra** framework. There are also additional plugins added to enable the project to be built and packaged as a deployable "fat jar".

```scala
logLevel := Level.Warn

resolvers += "Typesafe repository" at "http://repo.typesafe.com/typesafe/releases/"

addSbtPlugin( "com.eed3si9n" % "sbt-assembly" % "0.14.6" )
addSbtPlugin( "com.typesafe.sbt" % "sbt-native-packager" % "1.3.2" )
addSbtPlugin( "com.typesafe.sbt" % "sbt-twirl" % "1.3.13" )
addSbtPlugin( "org.scalatra.sbt" % "sbt-scalatra" % "1.0.2" )
```


`Dependencies.scala` is for dependency management. This centralizes all project dependencies and versions in one place. As mentioned above, this is overkill for a simple project like this, however, for complex multi-module builds it is invaluable in preventing configuration drift and dependency conflicts.
```scala
import sbt._

object Dependencies {

    val slf4jVersion = "1.7.5"
    val logbackVersion = "1.2.3"
    val sparkVersion = "2.3.0"
    val scalaTestVersion = "3.0.4"
    val scalatraVersion = "2.5.4"
    val jettyWebappVersion = "9.2.19.v20160908"
    val servletApiVersion = "3.1.0"
    val sparkTestBaseVersion = "2.2.0_0.8.0"

    val slf4j = Seq( "org.slf4j" % "slf4j-api" % slf4jVersion )

    val logback = Seq( "ch.qos.logback" % "logback-classic" % logbackVersion )

    val scalaTest = Seq( "org.scalatest" %% "scalatest" % scalaTestVersion % "test" )

    // TODO - fix versions later
    val scalatra = Seq( "org.scalatra" %% "scalatra" % scalatraVersion,
                        "org.scalatra" %% "scalatra-scalatest" % scalatraVersion % "test",
                        "org.eclipse.jetty" % "jetty-webapp" % jettyWebappVersion,
                        "javax.servlet" % "javax.servlet-api" % servletApiVersion )

    val spark = Seq( "org.apache.spark" %% "spark-core" % sparkVersion % "provided" )

    val sparkTestBase = Seq( "com.holdenkarau" %% "spark-testing-base" % sparkTestBaseVersion % "test" )


}
```

Finally, we come to the build definition itself, `build.sbt`. Comments have been added inline to the source code to explain the various components of the build.

```scala
import sbt._
import Dependencies._
organization := "io.radanalytics"
name := "tutorial-sparkpi-scala-scalatra"
version := "0.0.1-SNAPSHOT"
scalaVersion in ThisBuild := "2.11.11"

// 1. This is where SBT can reach out to resolve dependencies. SBT uses Apache Ivy to resolve dependencies by default, but can work with Maven repositories as well
resolvers += Resolver.sbtPluginRepo( "releases" )
resolvers += Classpaths.typesafeReleases
resolvers in ThisBuild ++= Seq( "Sonatype releases" at "https://oss.sonatype.org/content/repositories/releases",
                                "Spray IO Repository" at "http://repo.spray.io/",
                                "Maven Central" at "https://repo1.maven.org/maven2/",
                                "Typesafe repository" at "http://repo.typesafe.com/typesafe/releases/" )

// 2. Define the class to run when calling "java -jar ..."
mainClass in(Compile, run) := Some( "io.radanalytics.examples.scalatra.sparkpi.Main" )

// 3. Build the project to Java JAR conventions and add metadata to make Scalatra run
enablePlugins( JavaAppPackaging )
enablePlugins( ScalatraPlugin )


// 4. Add the project dependencies, see project/Dependencies.scala for dependency management
libraryDependencies ++= slf4j ++ logback ++ scalatra ++ scalaTest ++ spark ++ sparkTestBase


// 5. Deployment of this artifact should be part of a CI/CD pipeline. Running the unit tests while building the "fat jar" is very expensive,
//    therefore, don't do it during the "assembly" phase (which will be run on Openshift).
test in assembly := {}

// 6. Resolve any conflicts when merging into a "fat jar"
assemblyMergeStrategy in assembly := {
    case PathList( "META-INF", "MANIFEST.MF" ) => MergeStrategy.discard
    case PathList( "reference.conf" ) => MergeStrategy.concat
    //    case PathList( "META-INF", xs@_* ) => MergeStrategy.first
    case x => MergeStrategy.last
}
```

== Create the Spark Job
The code for this section is directly adapted from the https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala[Spark examples]. The job is encapsulated around a wrapper object that exposes just one method: calculate().

```scala
package io.radanalytics.examples.scalatra.sparkpi

import org.apache.spark.SparkContext

import scala.math.random

class SparkPI( spark : SparkContext, scale : Int ) {

    val applicationName = "Spark PI Scalatra Tutorial"

    def calculate( ) : Double = {
        val n = math.min( 100000L * scale, Int.MaxValue ).toInt // avoid overflow
        val count = spark.parallelize( 1 until n, scale ).map( i => {
            val x = random
            val y = random
            if ( x * x + y * y < 1 ) 1 else 0
        } ).reduce( _ + _ )
        4.0 * count / ( n - 1 )
    }

}
```

== Test the Spark Job
Thankfully, there is a library that will help enable the testing of Spark jobs in a unit test like environment. This framework, by using some utilities from https://github.com/apache/hadoop/tree/trunk/hadoop-minicluster[hadoop-minicluster], can stand up an entire Spark environment inside of a Scalatest fixture, execute jobs, and compare results. Unfortunately, this example relies on random numbers for it's computation, which makes it extremely hard to truly unit test. For real projects, https://github.com/holdenk/spark-testing-base[Spark Testing Base] includes a slew of testing capabilities including https://github.com/holdenk/spark-testing-base/wiki/RDDComparisons[RDD Comparisons], https://github.com/holdenk/spark-testing-base/wiki/DataFrameSuiteBase[Data Frame comparisons], and https://github.com/holdenk/spark-testing-base/wiki/StreamingSuiteBase[Spark Streaming test utilities].

For reference, a basic unit test would look something like this:
```scala
package io.radanalytics.examples.scalatra.sparkpi

import com.holdenkarau.spark.testing.SharedSparkContext
import org.scalatest.FlatSpec
import org.slf4j.{Logger, LoggerFactory}

class SparkPiTest extends FlatSpec with SharedSparkContext {

    val LOG : Logger = LoggerFactory.getLogger( this.getClass )

    "SparkPI" should "calculate to scale 2" in {
        val sparkPi : Double = new SparkPI( sc, 2 ).calculate()

        LOG.info( "--------------------------------------------" )
        LOG.info( s"---   Pi is roughly + $sparkPi" )
        LOG.info( "--------------------------------------------" )

        // NOTE - here is where you would put assertions, however, comparing floating point numbers that use random
        //        numbers in the algorithm is tricky so we don't do it here
        assert( true )
    }

}
```

== Implement the Service Endpoint
https://github.com/scalatra/scalatra[Scalatra] is designed from the ground up to be an easy to use microservice framework. It is based on the http://sinatrarb.com[similarly named Ruby framework], but with a Scala DSL and idioms. Setting up a SparkPI service is fairly easy and only requires the following:

1) Implemet a servlet to handle requests. This handler uses the **SparkPi** calculation that was implemented in the previous step.
```scala
package io.radanalytics.examples.scalatra.sparkpi

import org.apache.spark.{SparkConf, SparkContext}
import org.scalatra.{Ok, ScalatraServlet}

class SparkPiServlet extends ScalatraServlet {
    get( "/sparkpi" ) {
        val spark = new SparkContext( new SparkConf().setAppName( "Radanalytics IO Scalatra Tutorial" ) )
        val sparkPi = new SparkPI( spark,2 ).calculate()
        println( sparkPi )
        spark.stop()
        Ok( "Pi is roughly " + sparkPi )
    }
}
```

2) Scalatra initialization and plumbing. Scalatra will want to default this class to being called **ScalatraBootstrap** in the default package. This behavior will be overridden in the bootstrap of the application to encourage better code organization.
```
package io.radanalytics.examples.scalatra.sparkpi

import javax.servlet.ServletContext
import org.scalatra.LifeCycle

class ScalatraInit extends LifeCycle {

    override def init( context : ServletContext ) {
        context.mount( classOf[ SparkPiServlet ], "/*" )
    }

}
```
3) Implement the **Main** class, which will bootstrap Jetty and bind the **SparkPiServlet** so that it can handle requests. Notice the addition of an init parameter, which overrides the Scalatra default mentioned in the previous step.

```scala
package io.radanalytics.examples.scalatra.sparkpi

import org.eclipse.jetty.server.Server
import org.eclipse.jetty.servlet.DefaultServlet
import org.eclipse.jetty.webapp.WebAppContext
import org.scalatra.servlet.ScalatraListener

object Main {

    def main( args : Array[ String ] ) : Unit = {
        val port = 8080 //TODO - do I need to make the port configurable/dynamic?
        val server = new Server( port )
        val context = new WebAppContext()

        context.setContextPath( "/" )
        context.setResourceBase( "src/main/webapp" )
        context.setInitParameter( ScalatraListener.LifeCycleKey, "io.radanalytics.examples.scalatra.sparkpi.ScalatraInit" ) // Override the Scalatra default for ScalatraBootstrap in default package
        context.addEventListener( new ScalatraListener )
        context.addServlet( classOf[ DefaultServlet ], "/" ) // handles empty context root

        server.setHandler( context )
        server.start()
        server.join()
    }

}
```

== Build the Application and Verify Locally
At this point, your directory structure should look like this (steps for adding the logback configuration are not strictly required):

....
tutorial-sparkpi-scala-scalatra (root)
    \_ build.sbt
    \_ project
        \_ Dependencies.scala
        \_ build.properties
        \_ plugins.sbt
    \_ src
        \_ main
            \_ scala
                \_ io
                    \_ radanalytics
                        \_
                            \_ examples
                                \_ scalatra
                                    \_ sparkpi
                                        \_ Main.scala
                                        \_ ScalatraInit.scala
                                        \_ SparkPi.scala
                                        \_ SparkPiServlet.scala

    \_ src
        \_ main
            \_ scala
                \_ io
                    \_ radanalytics
                        \_
                            \_ examples
                                \_ scalatra
                                    \_ sparkpi
                                        \_ SparkPiTest.scala
....

Now you will run a local SBT build to ensure that your project compiles and all the tests pass.
```bash
# if this is your first SBT build this could take quite some time.
sbt clean test assembly
```

If this build succeeds, you should be able to verify that the deployable "fat jar" was built. It will be located as follows:
....
tutorial-sparkpi-scala-scalatra (root)
    \_ build.sbt
    ...
    \_ target
        \_ target
            \_scala-2.11
                \_ tutorial-sparkpi-scala-scalatra-assembly-0.0.1-SNAPSHOT.jar

....

You can test it out by running `java -jar target/scala-2.11/tutorial-sparkpi-scala-scalatra-assembly-0.0.1-SNAPSHOT.jar`. However, it will fail if you do not have a Spark cluster running locally.

== Deploy the Application to Openshift
Assuming you have created a project, imported the https://radanalytics.io/resources.yaml[Radanalytics resources], and created a cluster called **`spark`**, all you should need to do to deploy the application is execute the following command:
```bash
oc new-app --template oshinko-scala-spark-build-dc \
    -p APPLICATION_NAME=tutorial-sparkpi-scala-scalatra \
    -p GIT_URI=https://github.com/reynoldsm88/tutorial-sparkpi-scala-scalatra \
    -p APP_MAIN_CLASS=io.radanalytics.examples.scalatra.sparkpi.Main \
    -p APP_FILE=tutorial-sparkpi-scala-scalatra-assembly-0.0.1-SNAPSHOT.jar \
    -p SBT_ARGS="clean assembly" \
    -p APP_ARGS="-Xms1024M, -Xmx2048M, -XX:MaxMetaspace=1024M" \
    -p OSHINKO_CLUSTER_NAME="spark"
```

Use `oc logs -f bc/tutorial-sparkpi-scala-scalatra` to tail the logs. The assembly task from SBT will be run, so it could take awhile. Once the application is ready you can expose it with `oc expose svc/tutorial-sparkpi-scala-scalatra`.

Finally, you can get the IP address of your service endpoint. You can use this link in a web browser/REST client/curl command to submit a request and execute the Spark job.
```bash
   URL="http://$(oc get route tutorial-sparkpi-scala-scalatra | grep tutorial-sparkpi-scala-scalatra | awk '{print $2}')/sparkpi"
```

== Supplementary Materials and Further Reading
* The full source code of this example can be found https://github.com/reynoldsm88/tutorial-sparkpi-scala-scalatra[on GitHub]
* https://www.scala-sbt.org/1.x/docs/index.html[SBT documentation]
* https://github.com/holdenk/spark-testing-base/wiki[Spark Testing Base Documentation]
* http://www.scalatra.org/guides[Scalatra User Guides]
* http://www.scalatest.org[Scalatest Documentation]